arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 1.15.5
--------------------
git hash: b'62b1e0c7a4ede2deb8388de8933d08a7f487b159'
--------------------
b'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\nindex 7f98ca7..88e9052 100644\n--- a/src/align/detect_face.py\n+++ b/src/align/detect_face.py\n@@ -82,7 +82,7 @@ class Network(object):\n         session: The current TensorFlow session\n         ignore_missing: If true, serialized weights for missing layers are ignored.\n         """\n-        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n+        data_dict = np.load(data_path, encoding=\'latin1\',allow_pickle=True).item() #pylint: disable=no-member\n \n         for op_name in data_dict:\n             with tf.variable_scope(op_name, reuse=True):\ndiff --git a/src/classifier.py b/src/classifier.py\nindex e7189bc..b47b9c5 100644\n--- a/src/classifier.py\n+++ b/src/classifier.py\n@@ -1,19 +1,19 @@\n """An example of how to use your own dataset to train a classifier that recognizes people.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -36,135 +36,150 @@ import math\n import pickle\n from sklearn.svm import SVC\n \n+\n def main(args):\n-  \n+\n     with tf.Graph().as_default():\n-      \n-        with tf.Session() as sess:\n-            \n+\n+        with tf.compat.v1.Session() as sess:\n+\n             np.random.seed(seed=args.seed)\n-            \n+\n             if args.use_split_dataset:\n                 dataset_tmp = facenet.get_dataset(args.data_dir)\n-                train_set, test_set = split_dataset(dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n-                if (args.mode==\'TRAIN\'):\n+                train_set, test_set = split_dataset(\n+                    dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n+                if (args.mode == \'TRAIN\'):\n                     dataset = train_set\n-                elif (args.mode==\'CLASSIFY\'):\n+                elif (args.mode == \'CLASSIFY\'):\n                     dataset = test_set\n             else:\n                 dataset = facenet.get_dataset(args.data_dir)\n \n             # Check that there are at least one training image per class\n             for cls in dataset:\n-                assert(len(cls.image_paths)>0, \'There must be at least one image for each class in the dataset\')\n+                assert(len(cls.image_paths) > 0,\n+                       \'There must be at least one image for each class in the dataset\')\n \n-                 \n             paths, labels = facenet.get_image_paths_and_labels(dataset)\n-            \n+\n             print(\'Number of classes: %d\' % len(dataset))\n             print(\'Number of images: %d\' % len(paths))\n-            \n+\n             # Load the model\n             print(\'Loading feature extraction model\')\n             facenet.load_model(args.model)\n-            \n+\n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph(\n+            ).get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n-            \n+\n             # Run forward pass to calculate embeddings\n             print(\'Calculating features for images\')\n             nrof_images = len(paths)\n-            nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / args.batch_size))\n+            nrof_batches_per_epoch = int(\n+                math.ceil(1.0*nrof_images / args.batch_size))\n             emb_array = np.zeros((nrof_images, embedding_size))\n             for i in range(nrof_batches_per_epoch):\n                 start_index = i*args.batch_size\n                 end_index = min((i+1)*args.batch_size, nrof_images)\n                 paths_batch = paths[start_index:end_index]\n-                images = facenet.load_data(paths_batch, False, False, args.image_size)\n-                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n+                images = facenet.load_data(\n+                    paths_batch, False, False, args.image_size)\n+                feed_dict = {images_placeholder: images,\n+                             phase_train_placeholder: False}\n+                emb_array[start_index:end_index, :] = sess.run(\n+                    embeddings, feed_dict=feed_dict)\n+\n+            classifier_filename_exp = os.path.expanduser(\n+                args.classifier_filename)\n \n-            if (args.mode==\'TRAIN\'):\n+            if (args.mode == \'TRAIN\'):\n                 # Train classifier\n                 print(\'Training classifier\')\n                 model = SVC(kernel=\'linear\', probability=True)\n                 model.fit(emb_array, labels)\n-            \n+\n                 # Create a list of class names\n-                class_names = [ cls.name.replace(\'_\', \' \') for cls in dataset]\n+                class_names = [cls.name.replace(\'_\', \' \') for cls in dataset]\n \n                 # Saving classifier model\n                 with open(classifier_filename_exp, \'wb\') as outfile:\n                     pickle.dump((model, class_names), outfile)\n-                print(\'Saved classifier model to file "%s"\' % classifier_filename_exp)\n-                \n-            elif (args.mode==\'CLASSIFY\'):\n+                print(\'Saved classifier model to file "%s"\' %\n+                      classifier_filename_exp)\n+\n+            elif (args.mode == \'CLASSIFY\'):\n                 # Classify images\n                 print(\'Testing classifier\')\n                 with open(classifier_filename_exp, \'rb\') as infile:\n                     (model, class_names) = pickle.load(infile)\n \n-                print(\'Loaded classifier model from file "%s"\' % classifier_filename_exp)\n+                print(\'Loaded classifier model from file "%s"\' %\n+                      classifier_filename_exp)\n \n                 predictions = model.predict_proba(emb_array)\n                 best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                \n+                best_class_probabilities = predictions[np.arange(\n+                    len(best_class_indices)), best_class_indices]\n+\n                 for i in range(len(best_class_indices)):\n-                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n-                    \n+                    print(\'%4d  %s: %.3f\' % (\n+                        i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n+\n                 accuracy = np.mean(np.equal(best_class_indices, labels))\n                 print(\'Accuracy: %.3f\' % accuracy)\n-                \n-            \n+\n+\n def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n     train_set = []\n     test_set = []\n     for cls in dataset:\n         paths = cls.image_paths\n         # Remove classes with less than min_nrof_images_per_class\n-        if len(paths)>=min_nrof_images_per_class:\n+        if len(paths) >= min_nrof_images_per_class:\n             np.random.shuffle(paths)\n-            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n-            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n+            train_set.append(facenet.ImageClass(\n+                cls.name, paths[:nrof_train_images_per_class]))\n+            test_set.append(facenet.ImageClass(\n+                cls.name, paths[nrof_train_images_per_class:]))\n     return train_set, test_set\n \n-            \n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n+\n     parser.add_argument(\'mode\', type=str, choices=[\'TRAIN\', \'CLASSIFY\'],\n-        help=\'Indicates if a new classifier should be trained or a classification \' + \n-        \'model should be used for classification\', default=\'CLASSIFY\')\n+                        help=\'Indicates if a new classifier should be trained or a classification \' +\n+                        \'model should be used for classification\', default=\'CLASSIFY\')\n     parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'classifier_filename\', \n-        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n-        \'For training this is the output and for classification this is an input.\')\n-    parser.add_argument(\'--use_split_dataset\', \n-        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +  \n-        \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n+                        help=\'Path to the data directory containing aligned LFW face patches.\')\n+    parser.add_argument(\'model\', type=str,\n+                        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n+    parser.add_argument(\'classifier_filename\',\n+                        help=\'Classifier model file name as a pickle (.pkl) file. \' +\n+                        \'For training this is the output and for classification this is an input.\')\n+    parser.add_argument(\'--use_split_dataset\',\n+                        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +\n+                        \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n     parser.add_argument(\'--test_data_dir\', type=str,\n-        help=\'Path to the test data directory containing aligned images used for testing.\')\n+                        help=\'Path to the test data directory containing aligned images used for testing.\')\n     parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n+                        help=\'Number of images to process in a batch.\', default=90)\n     parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n+                        help=\'Image size (height, width) in pixels.\', default=160)\n     parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n+                        help=\'Random seed.\', default=666)\n     parser.add_argument(\'--min_nrof_images_per_class\', type=int,\n-        help=\'Only include classes with at least this number of images in the dataset\', default=20)\n+                        help=\'Only include classes with at least this number of images in the dataset\', default=20)\n     parser.add_argument(\'--nrof_train_images_per_class\', type=int,\n-        help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n-    \n+                        help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n+\n     return parser.parse_args(argv)\n \n+\n if __name__ == \'__main__\':\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\nindex cfbd4f4..c7b47d5 100644\n--- a/src/face_rec_cam.py\n+++ b/src/face_rec_cam.py\n@@ -22,7 +22,8 @@ from sklearn.svm import SVC\n \n def main():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n+    parser.add_argument(\n+        \'--path\', help=\'Path of the video you want to test on.\', default=0)\n     args = parser.parse_args()\n \n     MINSIZE = 20\n@@ -42,7 +43,8 @@ def main():\n     with tf.Graph().as_default():\n \n         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess = tf.Session(config=tf.ConfigProto(\n+            gpu_options=gpu_options, log_device_placement=False))\n \n         with sess.as_default():\n \n@@ -56,19 +58,21 @@ def main():\n             phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n \n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n+            pnet, rnet, onet = align.detect_face.create_mtcnn(\n+                sess, "src/align")\n \n             people_detected = set()\n             person_detected = collections.Counter()\n \n-            cap  = VideoStream(src=0).start()\n+            cap = VideoStream(src=0).start()\n \n             while (True):\n                 frame = cap.read()\n                 frame = imutils.resize(frame, width=600)\n                 frame = cv2.flip(frame, 1)\n \n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n+                bounding_boxes, _ = align.detect_face.detect_face(\n+                    frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n \n                 faces_found = bounding_boxes.shape[0]\n                 try:\n@@ -86,26 +90,39 @@ def main():\n                             print(bb[i][3]-bb[i][1])\n                             print(frame.shape[0])\n                             print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n+                            if (bb[i][3]-bb[i][1])/frame.shape[0] > 0.25:\n+                                cropped = frame[bb[i][1]:bb[i]\n+                                                [3], bb[i][0]:bb[i][2], :]\n                                 scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n                                                     interpolation=cv2.INTER_CUBIC)\n                                 scaled = facenet.prewhiten(scaled)\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n+                                scaled_reshape = scaled.reshape(\n+                                    -1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n+                                feed_dict = {\n+                                    images_placeholder: scaled_reshape, phase_train_placeholder: False}\n+                                emb_array = sess.run(\n+                                    embeddings, feed_dict=feed_dict)\n \n                                 predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(predictions, axis=1)\n+                                best_class_indices = np.argmax(\n+                                    predictions, axis=1)\n                                 best_class_probabilities = predictions[\n                                     np.arange(len(best_class_indices)), best_class_indices]\n                                 best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n+                                print("Name: {}, Probability: {}".format(\n+                                    best_name, best_class_probabilities))\n \n+                                file_path = \'result.txt\'\n+                                file_handle = open(file_path, \'w\')\n+                                text2 = \'Name: {}, Probability: {}\'.format(\n+                                    best_name, best_class_probabilities)\n \n+                                file_handle.write(text2)\n+                                file_handle.close()\n \n                                 if best_class_probabilities > 0.8:\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                                    cv2.rectangle(\n+                                        frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n                                     text_x = bb[i][0]\n                                     text_y = bb[i][3] + 20\n \n@@ -130,4 +147,4 @@ def main():\n             cv2.destroyAllWindows()\n \n \n-main()\n\\ No newline at end of file\n+main()\ndiff --git a/src/facenet.py b/src/facenet.py\nindex bfe6802..5b93612 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -1,19 +1,19 @@\n """Functions for building the face recognition network.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.p\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -41,33 +41,35 @@ from tensorflow.python.platform import gfile\n import math\n from six import iteritems\n \n+\n def triplet_loss(anchor, positive, negative, alpha):\n     """Calculate the triplet loss according to the FaceNet paper\n-    \n+\n     Args:\n       anchor: the embeddings for the anchor images.\n       positive: the embeddings for the positive images.\n       negative: the embeddings for the negative images.\n-  \n+\n     Returns:\n       the triplet loss according to the FaceNet paper as a float tensor.\n     """\n     with tf.variable_scope(\'triplet_loss\'):\n         pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n         neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n+\n+        basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n         loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n+\n     return loss\n-  \n+\n+\n def center_loss(features, label, alfa, nrof_classes):\n     """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n        (http://ydwen.github.io/papers/WenECCV16.pdf)\n     """\n     nrof_features = features.get_shape()[1]\n     centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n+                              initializer=tf.constant_initializer(0), trainable=False)\n     label = tf.reshape(label, [-1])\n     centers_batch = tf.gather(centers, label)\n     diff = (1 - alfa) * (centers_batch - features)\n@@ -76,6 +78,7 @@ def center_loss(features, label, alfa, nrof_classes):\n         loss = tf.reduce_mean(tf.square(features - centers_batch))\n     return loss, centers\n \n+\n def get_image_paths_and_labels(dataset):\n     image_paths_flat = []\n     labels_flat = []\n@@ -84,22 +87,27 @@ def get_image_paths_and_labels(dataset):\n         labels_flat += [i] * len(dataset[i].image_paths)\n     return image_paths_flat, labels_flat\n \n+\n def shuffle_examples(image_paths, labels):\n     shuffle_list = list(zip(image_paths, labels))\n     random.shuffle(shuffle_list)\n     image_paths_shuff, labels_shuff = zip(*shuffle_list)\n     return image_paths_shuff, labels_shuff\n \n+\n def random_rotate_image(image):\n     angle = np.random.uniform(low=-10.0, high=10.0)\n     return misc.imrotate(image, angle, \'bicubic\')\n-  \n+\n+\n # 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n RANDOM_ROTATE = 1\n RANDOM_CROP = 2\n RANDOM_FLIP = 4\n FIXED_STANDARDIZATION = 8\n FLIP = 16\n+\n+\n def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n     images_and_labels_list = []\n     for _ in range(nrof_preprocess_threads):\n@@ -109,42 +117,45 @@ def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batc\n             file_contents = tf.read_file(filename)\n             image = tf.image.decode_image(file_contents, 3)\n             image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n+                            lambda: tf.py_func(random_rotate_image, [\n+                                               image], tf.uint8),\n+                            lambda: tf.identity(image))\n+            image = tf.cond(get_control_flag(control[0], RANDOM_CROP),\n+                            lambda: tf.random_crop(image, image_size + (3,)),\n+                            lambda: tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n             image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n+                            lambda: tf.image.random_flip_left_right(image),\n+                            lambda: tf.identity(image))\n             image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n+                            lambda: (tf.cast(image, tf.float32) - 127.5)/128.0,\n+                            lambda: tf.image.per_image_standardization(image))\n             image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n+                            lambda: tf.image.flip_left_right(image),\n+                            lambda: tf.identity(image))\n             #pylint: disable=no-member\n             image.set_shape(image_size + (3,))\n             images.append(image)\n         images_and_labels_list.append([images, label])\n \n     image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n+        images_and_labels_list, batch_size=batch_size_placeholder,\n         shapes=[image_size + (3,), ()], enqueue_many=True,\n         capacity=4 * nrof_preprocess_threads * 100,\n         allow_smaller_final_batch=True)\n-    \n+\n     return image_batch, label_batch\n \n+\n def get_control_flag(control, field):\n     return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n-  \n+\n+\n def _add_loss_summaries(total_loss):\n     """Add summaries for losses.\n-  \n+\n     Generates moving average for all losses and associated summaries for\n     visualizing the performance of the network.\n-  \n+\n     Args:\n       total_loss: Total loss from loss().\n     Returns:\n@@ -154,92 +165,103 @@ def _add_loss_summaries(total_loss):\n     loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n     losses = tf.get_collection(\'losses\')\n     loss_averages_op = loss_averages.apply(losses + [total_loss])\n-  \n+\n     # Attach a scalar summmary to all individual losses and the total loss; do the\n     # same for the averaged version of the losses.\n     for l in losses + [total_loss]:\n         # Name each loss as \'(raw)\' and name the moving average version of the loss\n         # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n+        tf.summary.scalar(l.op.name + \' (raw)\', l)\n         tf.summary.scalar(l.op.name, loss_averages.average(l))\n-  \n+\n     return loss_averages_op\n \n+\n def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n     # Generate moving averages of all losses and associated summaries.\n     loss_averages_op = _add_loss_summaries(total_loss)\n \n     # Compute gradients.\n     with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n+        if optimizer == \'ADAGRAD\':\n             opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n+        elif optimizer == \'ADADELTA\':\n+            opt = tf.train.AdadeltaOptimizer(\n+                learning_rate, rho=0.9, epsilon=1e-6)\n+        elif optimizer == \'ADAM\':\n+            opt = tf.train.AdamOptimizer(\n+                learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n+        elif optimizer == \'RMSPROP\':\n+            opt = tf.train.RMSPropOptimizer(\n+                learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n+        elif optimizer == \'MOM\':\n+            opt = tf.train.MomentumOptimizer(\n+                learning_rate, 0.9, use_nesterov=True)\n         else:\n             raise ValueError(\'Invalid optimization algorithm\')\n-    \n+\n         grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n+\n     # Apply gradients.\n     apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n-  \n+\n     # Add histograms for trainable variables.\n     if log_histograms:\n         for var in tf.trainable_variables():\n             tf.summary.histogram(var.op.name, var)\n-   \n+\n     # Add histograms for gradients.\n     if log_histograms:\n         for grad, var in grads:\n             if grad is not None:\n                 tf.summary.histogram(var.op.name + \'/gradients\', grad)\n-  \n+\n     # Track the moving averages of all trainable variables.\n     variable_averages = tf.train.ExponentialMovingAverage(\n         moving_average_decay, global_step)\n     variables_averages_op = variable_averages.apply(tf.trainable_variables())\n-  \n+\n     with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n         train_op = tf.no_op(name=\'train\')\n-  \n+\n     return train_op\n \n+\n def prewhiten(x):\n     mean = np.mean(x)\n     std = np.std(x)\n     std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n     y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n+    return y\n+\n \n def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n+    if image.shape[1] > image_size:\n         sz1 = int(image.shape[1]//2)\n         sz2 = int(image_size//2)\n         if random_crop:\n             diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n+            (h, v) = (np.random.randint(-diff, diff+1),\n+                      np.random.randint(-diff, diff+1))\n         else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n+            (h, v) = (0, 0)\n+        image = image[(sz1-sz2+v):(sz1+sz2+v), (sz1-sz2+h):(sz1+sz2+h), :]\n     return image\n-  \n+\n+\n def flip(image, random_flip):\n     if random_flip and np.random.choice([True, False]):\n         image = np.fliplr(image)\n     return image\n \n+\n def to_rgb(img):\n     w, h = img.shape\n     ret = np.empty((w, h, 3), dtype=np.uint8)\n     ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n     return ret\n-  \n+\n+\n def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n     nrof_samples = len(image_paths)\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\n@@ -251,33 +273,36 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\n             img = prewhiten(img)\n         img = crop(img, do_random_crop, image_size)\n         img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n+        images[i, :, :, :] = img\n     return images\n \n+\n def get_label_batch(label_data, batch_size, batch_index):\n     nrof_examples = np.size(label_data, 0)\n     j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n+    if j+batch_size <= nrof_examples:\n         batch = label_data[j:j+batch_size]\n     else:\n         x1 = label_data[j:nrof_examples]\n         x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n+        batch = np.vstack([x1, x2])\n     batch_int = batch.astype(np.int64)\n     return batch_int\n \n+\n def get_batch(image_data, batch_size, batch_index):\n     nrof_examples = np.size(image_data, 0)\n     j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n+    if j+batch_size <= nrof_examples:\n+        batch = image_data[j:j+batch_size, :, :, :]\n     else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n+        x1 = image_data[j:nrof_examples, :, :, :]\n+        x2 = image_data[0:nrof_examples-j, :, :, :]\n+        batch = np.vstack([x1, x2])\n     batch_float = batch.astype(np.float32)\n     return batch_float\n \n+\n def get_triplet_batch(triplets, batch_index, batch_size):\n     ax, px, nx = triplets\n     a = get_batch(ax, int(batch_size/3), batch_index)\n@@ -286,6 +311,7 @@ def get_triplet_batch(triplets, batch_index, batch_size):\n     batch = np.vstack([a, p, n])\n     return batch\n \n+\n def get_learning_rate_from_file(filename, epoch):\n     with open(filename, \'r\') as f:\n         for line in f.readlines():\n@@ -293,7 +319,7 @@ def get_learning_rate_from_file(filename, epoch):\n             if line:\n                 par = line.strip().split(\':\')\n                 e = int(par[0])\n-                if par[1]==\'-\':\n+                if par[1] == \'-\':\n                     lr = -1\n                 else:\n                     lr = float(par[1])\n@@ -302,23 +328,26 @@ def get_learning_rate_from_file(filename, epoch):\n                 else:\n                     return learning_rate\n \n+\n class ImageClass():\n     "Stores the paths to images for a given class"\n+\n     def __init__(self, name, image_paths):\n         self.name = name\n         self.image_paths = image_paths\n-  \n+\n     def __str__(self):\n         return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n-  \n+\n     def __len__(self):\n         return len(self.image_paths)\n-  \n+\n+\n def get_dataset(path, has_class_directories=True):\n     dataset = []\n     path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n+    classes = [path for path in os.listdir(path_exp)\n+               if os.path.isdir(os.path.join(path_exp, path))]\n     classes.sort()\n     nrof_classes = len(classes)\n     for i in range(nrof_classes):\n@@ -326,25 +355,27 @@ def get_dataset(path, has_class_directories=True):\n         facedir = os.path.join(path_exp, class_name)\n         image_paths = get_image_paths(facedir)\n         dataset.append(ImageClass(class_name, image_paths))\n-  \n+\n     return dataset\n \n+\n def get_image_paths(facedir):\n     image_paths = []\n     if os.path.isdir(facedir):\n         images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n+        image_paths = [os.path.join(facedir, img) for img in images]\n     return image_paths\n-  \n+\n+\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n+    if mode == \'SPLIT_CLASSES\':\n         nrof_classes = len(dataset)\n         class_indices = np.arange(nrof_classes)\n         np.random.shuffle(class_indices)\n         split = int(round(nrof_classes*(1-split_ratio)))\n         train_set = [dataset[i] for i in class_indices[0:split]]\n         test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n+    elif mode == \'SPLIT_IMAGES\':\n         train_set = []\n         test_set = []\n         for cls in dataset:\n@@ -352,42 +383,48 @@ def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n             np.random.shuffle(paths)\n             nrof_images_in_class = len(paths)\n             split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n+            if split == nrof_images_in_class:\n                 split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n+            if split >= min_nrof_images_per_class and nrof_images_in_class-split >= 1:\n                 train_set.append(ImageClass(cls.name, paths[:split]))\n                 test_set.append(ImageClass(cls.name, paths[split:]))\n     else:\n         raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n     return train_set, test_set\n \n+\n def load_model(model, input_map=None):\n     # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n     #  or if it is a protobuf file with a frozen graph\n     model_exp = os.path.expanduser(model)\n     if (os.path.isfile(model_exp)):\n         print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n+        with tf.io.gfile.GFile(model_exp, \'rb\') as f:\n+            graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n             tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n     else:\n         print(\'Model directory: %s\' % model_exp)\n         meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n+\n         print(\'Metagraph file: %s\' % meta_file)\n         print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n+\n+        saver = tf.train.import_meta_graph(os.path.join(\n+            model_exp, meta_file), input_map=input_map)\n+        saver.restore(tf.get_default_session(),\n+                      os.path.join(model_exp, ckpt_file))\n+\n+\n def get_model_filenames(model_dir):\n     files = os.listdir(model_dir)\n     meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n+    if len(meta_files) == 0:\n+        raise ValueError(\n+            \'No meta file found in the model directory (%s)\' % model_dir)\n+    elif len(meta_files) > 1:\n+        raise ValueError(\n+            \'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n     meta_file = meta_files[0]\n     ckpt = tf.train.get_checkpoint_state(model_dir)\n     if ckpt and ckpt.model_checkpoint_path:\n@@ -398,107 +435,118 @@ def get_model_filenames(model_dir):\n     max_step = -1\n     for f in files:\n         step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n+        if step_str is not None and len(step_str.groups()) >= 2:\n             step = int(step_str.groups()[1])\n             if step > max_step:\n                 max_step = step\n                 ckpt_file = step_str.groups()[0]\n     return meta_file, ckpt_file\n-  \n+\n+\n def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n+    if distance_metric == 0:\n         # Euclidian distance\n         diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n+        dist = np.sum(np.square(diff), 1)\n+    elif distance_metric == 1:\n         # Distance based on cosine similarity\n         dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n+        norm = np.linalg.norm(embeddings1, axis=1) * \\\n+            np.linalg.norm(embeddings2, axis=1)\n         similarity = dot / norm\n         dist = np.arccos(similarity) / math.pi\n     else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n+        raise \'Undefined distance metric %d\' % distance_metric\n+\n     return dist\n \n+\n def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n     assert(embeddings1.shape[0] == embeddings2.shape[0])\n     assert(embeddings1.shape[1] == embeddings2.shape[1])\n     nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n     nrof_thresholds = len(thresholds)\n     k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n+\n+    tprs = np.zeros((nrof_folds, nrof_thresholds))\n+    fprs = np.zeros((nrof_folds, nrof_thresholds))\n     accuracy = np.zeros((nrof_folds))\n-    \n+\n     indices = np.arange(nrof_pairs)\n-    \n+\n     for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n         if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+            mean = np.mean(np.concatenate(\n+                [embeddings1[train_set], embeddings2[train_set]]), axis=0)\n         else:\n-          mean = 0.0\n+            mean = 0.0\n         dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n+\n         # Find the best threshold for the fold\n         acc_train = np.zeros((nrof_thresholds))\n         for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n+            _, _, acc_train[threshold_idx] = calculate_accuracy(\n+                threshold, dist[train_set], actual_issame[train_set])\n         best_threshold_index = np.argmax(acc_train)\n         for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n+            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(\n+                threshold, dist[test_set], actual_issame[test_set])\n+        _, _, accuracy[fold_idx] = calculate_accuracy(\n+            thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n+\n+        tpr = np.mean(tprs, 0)\n+        fpr = np.mean(fprs, 0)\n     return tpr, fpr, accuracy\n \n+\n def calculate_accuracy(threshold, dist, actual_issame):\n     predict_issame = np.less(dist, threshold)\n     tp = np.sum(np.logical_and(predict_issame, actual_issame))\n     fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n+    tn = np.sum(np.logical_and(np.logical_not(\n+        predict_issame), np.logical_not(actual_issame)))\n     fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n-  \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n+\n+    tpr = 0 if (tp+fn == 0) else float(tp) / float(tp+fn)\n+    fpr = 0 if (fp+tn == 0) else float(fp) / float(fp+tn)\n     acc = float(tp+tn)/dist.size\n     return tpr, fpr, acc\n \n \n-  \n def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n     assert(embeddings1.shape[0] == embeddings2.shape[0])\n     assert(embeddings1.shape[1] == embeddings2.shape[1])\n     nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n     nrof_thresholds = len(thresholds)\n     k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n+\n     val = np.zeros(nrof_folds)\n     far = np.zeros(nrof_folds)\n-    \n+\n     indices = np.arange(nrof_pairs)\n-    \n+\n     for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n         if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+            mean = np.mean(np.concatenate(\n+                [embeddings1[train_set], embeddings2[train_set]]), axis=0)\n         else:\n-          mean = 0.0\n+            mean = 0.0\n         dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n+\n         # Find the threshold that gives FAR = far_target\n         far_train = np.zeros(nrof_thresholds)\n         for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n+            _, far_train[threshold_idx] = calculate_val_far(\n+                threshold, dist[train_set], actual_issame[train_set])\n+        if np.max(far_train) >= far_target:\n             f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n             threshold = f(far_target)\n         else:\n             threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n-  \n+\n+        val[fold_idx], far[fold_idx] = calculate_val_far(\n+            threshold, dist[test_set], actual_issame[test_set])\n+\n     val_mean = np.mean(val)\n     far_mean = np.mean(far)\n     val_std = np.std(val)\n@@ -508,63 +556,71 @@ def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_targe\n def calculate_val_far(threshold, dist, actual_issame):\n     predict_issame = np.less(dist, threshold)\n     true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n+    false_accept = np.sum(np.logical_and(\n+        predict_issame, np.logical_not(actual_issame)))\n     n_same = np.sum(actual_issame)\n     n_diff = np.sum(np.logical_not(actual_issame))\n     val = float(true_accept) / float(n_same)\n     far = float(false_accept) / float(n_diff)\n     return val, far\n \n+\n def store_revision_info(src_path, output_dir, arg_string):\n     try:\n         # Get git hash\n         cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+        gitproc = Popen(cmd, stdout=PIPE, cwd=src_path)\n         (stdout, _) = gitproc.communicate()\n         git_hash = stdout.strip()\n     except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n-  \n+        git_hash = \' \'.join(cmd) + \': \' + e.strerror\n+\n     try:\n         # Get local changes\n         cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+        gitproc = Popen(cmd, stdout=PIPE, cwd=src_path)\n         (stdout, _) = gitproc.communicate()\n         git_diff = stdout.strip()\n     except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n+        git_diff = \' \'.join(cmd) + \': \' + e.strerror\n+\n     # Store a text file in the log directory\n     rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n     with open(rev_info_filename, "w") as text_file:\n         text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n+        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' %\n+                        tf.__version__)  # @UndefinedVariable\n         text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n         text_file.write(\'%s\' % git_diff)\n \n+\n def list_variables(filename):\n     reader = training.NewCheckpointReader(filename)\n     variable_map = reader.get_variable_to_shape_map()\n     names = sorted(variable_map.keys())\n     return names\n \n-def put_images_on_grid(images, shape=(16,8)):\n+\n+def put_images_on_grid(images, shape=(16, 8)):\n     nrof_images = images.shape[0]\n     img_size = images.shape[1]\n     bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n+    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]\n+                   * (img_size+bw)+bw, 3), np.float32)\n     for i in range(shape[1]):\n         x_start = i*(img_size+bw)+bw\n         for j in range(shape[0]):\n             img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n+            if img_index >= nrof_images:\n                 break\n             y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n+            img[x_start:x_start+img_size, y_start:y_start +\n+                img_size, :] = images[img_index, :, :, :]\n+        if img_index >= nrof_images:\n             break\n     return img\n \n+\n def write_arguments_to_file(args, filename):\n     with open(filename, \'w\') as f:\n         for key, value in iteritems(vars(args)):'